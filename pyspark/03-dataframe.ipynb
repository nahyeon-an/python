{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://namenode:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.56.100:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://192.168.56.100:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json 형식의 RDD를 생성한다.  \n",
    "트랜스포메이션 연산이므로 spark.read.json에서 호출될 때 실행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringJsonRdd = sc.parallelize((\n",
    "    \"\"\"{\"id\": \"123\", \"name\": \"Katie\", \"age\": 19, \"eyeColor\": \"brown\"}\"\"\",\n",
    "    \"\"\"{\"id\": \"234\", \"name\": \"Michael\", \"age\": 22, \"eyeColor\": \"green\"}\"\"\",\n",
    "    \"\"\"{\"id\": \"345\", \"name\": \"Simone\", \"age\": 29, \"eyeColor\": \"blue\"}\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe 생성\n",
    "swimmerJson = spark.read.json(stringJsonRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "swimmerJson dataframe의 임시 테이블을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmerJson.createOrReplaceTempView(\"swimmerJson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame API 1. show(n)  \n",
    "n개의 행을 출력한다. 디폴트 n == 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 29|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerJson.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark는 pyspark shell에서 자동으로 생성된 SparkSession이다.  \n",
    "collect() 함수는 모든 데이터를 행 객체로 변환한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=29, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from swimmerJson\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 RDD를 Dataframe으로 변경하는 방법 1.  \n",
    "Reflection을 이용한 스키마 추측  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL 은 행 객체 RDD를 데이터프레임으로 변경한다.  \n",
    "이때 데이터 타입을 데이터 샘플링을 통해 추측하여 자동으로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmerJson.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 RDD를 Dataframe으로 변경하는 방법 2.  \n",
    "스키마를 직접 명시하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringCsvRdd = sc.parallelize([\n",
    "    (123, 'Katie', 19, 'brown'),\n",
    "    (234, 'Michael', 23, 'green'),\n",
    "    (345, 'Simone', 29, 'blue')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StructField(name, dataType, nullable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd에 정의한 스키마를 적용하여 데이터프레임을 생성\n",
    "swimmers = spark.createDataFrame(stringCsvRdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임 API로 쿼리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()  # 데이터프레임의 행의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 23|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select : 리턴할 컬럼 지정\n",
    "# filter : 조건 설정\n",
    "swimmers.select(\"id\", \"age\").filter(\"age=23\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 23|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 23).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eyeColor가 b로 시작하는 데이터 조회\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL로 쿼리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sql문을 통한 행의 개수 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 23|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, age from swimmers where age = 23\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|eyeColor|\n",
      "+---+--------+\n",
      "|123|   brown|\n",
      "|345|    blue|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Senario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightFilePath = \"file:///home/an/nb-workspace/flight-data/departuredelays.csv\"\n",
    "airportFilePath = \"file:///home/an/nb-workspace/flight-data/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = spark.read.csv(airportFilePath, header='true', inferSchema='true', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fightPerf = spark.read.csv(flightFilePath, header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fightPerf.createOrReplaceTempView(\"FlightPerformance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select a.City, f.origin, sum(f.delay) as Delays\n",
    "from FlightPerformance f\n",
    "join airports a\n",
    "on a.IATA = f.origin\n",
    "where a.State = 'WA'\n",
    "group by a.City, f.origin\n",
    "order by sum(f.delay) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|State|   Delays|\n",
      "+-----+---------+\n",
      "|   SC|  80666.0|\n",
      "|   AZ| 401793.0|\n",
      "|   LA| 199136.0|\n",
      "|   MN| 256811.0|\n",
      "|   NJ| 452791.0|\n",
      "|   OR| 109333.0|\n",
      "|   VA|  98016.0|\n",
      "| null| 397237.0|\n",
      "|   RI|  30760.0|\n",
      "|   WY|  15365.0|\n",
      "|   KY|  61156.0|\n",
      "|   NH|  20474.0|\n",
      "|   MI| 366486.0|\n",
      "|   NV| 474208.0|\n",
      "|   WI| 152311.0|\n",
      "|   ID|  22932.0|\n",
      "|   CA|1891919.0|\n",
      "|   CT|  54662.0|\n",
      "|   NE|  59376.0|\n",
      "|   MT|  19271.0|\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select a.State, sum(f.delay) as Delays\n",
    "from FlightPerformance f\n",
    "join airports a\n",
    "on a.IATA = f.origin\n",
    "where a.Country = 'USA'\n",
    "group by a.State\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
