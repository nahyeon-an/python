{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 검사, 모니터링 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 epoch로 훈련을 하다가 overfitting 이 발생하는 지점을 찾고 해당 epoch으로 훈련하는 것 = 낭비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 손실이 향상되지 않을 때 훈련을 멈춘다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "콜백을 사용하는 경우  \n",
    "- 모델의 checkpoint 저장 : 모델의 현재 가중치를 저장  \n",
    "- early stopping : 더 이상 향상되지 않을 때 훈련 중지  \n",
    "- hyper-parameter 를 동적으로 조정 : 학습률 같은 것을 조정\n",
    "- 훈련, 검증 지표를 로그에 기록 or 표현이 업데이트 될 때마다 시각화 : keras progress bar 도 하나의 콜백  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelCheckpoint 콜백"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 동안 모델을 저장할 때 사용  \n",
    "지금까지 가장 좋은 모델만 저장할 수 있음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarlyStopping 콜백"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정해진 에포크 동안 모니터링 지표가 향상되지 않으면 훈련을 중지  \n",
    "overfitting 발생하면 훈련 중지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks_list = [\n",
    "    # patience : 1 epoch 보다 길게 = 에포크 2 이상 동안 monitor 지표가 향상되지 않으면 훈련 중지\n",
    "    EarlyStopping(monitor='val_acc', patience=1),\n",
    "    # val_loss 가 가장 좋은 모델만 저장\n",
    "    ModelCheckpoint(filepath='my_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(x, y, \n",
    "          epochs=10, \n",
    "          batch_size=32, \n",
    "          callbacks=callbacks_list, # callback list 전달\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau 콜백"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 손실이 향상되지 않을 때 학습률을 작게 조정  \n",
    "지역 최솟값에서 빠져나올 수 있음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "callbacks_list = [\n",
    "    # val_loss 를 모니터링\n",
    "    # 검증 손실값이 10에포크 동안 좋아지지 않으면 콜백 호출\n",
    "    # learning rate 에 0.1 을 곱함 = 학습률을 10배로 줄인다\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "]\n",
    "\n",
    "model.fit(x, y, \n",
    "          epochs=10, \n",
    "          batch_size=32, \n",
    "          callbacks=callbacks_list, # callback list 전달\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 콜백 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.callbacks.Callback을 상속받아 구현  \n",
    "\n",
    "약속된 메서드를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    \n",
    "    # 훈련 모델에 대한 정보를 전달하기 위해 훈련 전에 호출됨\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        self.activations_model = keras.model.Model(model.input, layer_ouputs)  # 각 층의 활성화 출력을 반환\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data')\n",
    "        \n",
    "        # 검증 데이터의 첫번째 샘플\n",
    "        validation_sample = self.validation_data[0][0:1]\n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        f = open('activation_at_epoch_' + str(epoch) + '.npz', 'wb')\n",
    "        np.savez(f, activations)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "브라우저 기반의 시각화 도구  \n",
    "텐서플로 백엔드로 케라스를 설정한 경우에만 사용 가능  \n",
    "\n",
    "목적 : 훈련 모델의 내부에서 일어나는 모든 것을 시각적으로 모니터링  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/Users/nahyeonan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/nahyeonan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "imdb (InputLayer)            [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, None, 128)         256000    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 32)          28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 32)          7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, Input\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "\n",
    "max_features = 2000\n",
    "max_len = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "imdb_input = Input(shape=(None,), dtype='int32', name='imdb')\n",
    "embedding = layers.Embedding(max_features, 128, input_length=max_len, name='embed')(imdb_input)\n",
    "x = layers.Conv1D(32, 7, activation='relu')(embedding)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(32, 7, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "output = layers.Dense(1)(x)\n",
    "\n",
    "model = Model([imdb_input], output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_model = keras.models.Sequential()\n",
    "seq_model.add(layers.Embedding(max_features, 128, \n",
    "                               input_length=max_len, \n",
    "                               name='embed'))\n",
    "seq_model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "seq_model.add(layers.MaxPooling1D(5))\n",
    "seq_model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "seq_model.add(layers.GlobalMaxPooling1D())\n",
    "seq_model.add(layers.Dense(1))\n",
    "\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서보드 로그 파일을 기록하는 디렉터리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir my_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard Callback  \n",
    "지정된 디스크 위치에 로그 이벤트를 기록  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='my_log_dir',  # 로그파일 저장 위치\n",
    "        histogram_freq=1,  # 1 에포크마다 활성화 출력의 히스토그램 기록\n",
    "        embeddings_freq=1  # 1 에포크마다 임베딩 데이터 기록\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='my_model.h5', \n",
    "        monitor='val_loss', \n",
    "        save_best_only=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.1, \n",
    "        patience=4\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 - 18s - loss: 0.6381 - acc: 0.6913 - val_loss: 0.4696 - val_acc: 0.8168\n",
      "Epoch 2/20\n",
      "157/157 - 18s - loss: 0.4465 - acc: 0.8561 - val_loss: 0.7135 - val_acc: 0.8098\n",
      "Epoch 3/20\n",
      "157/157 - 18s - loss: 0.3680 - acc: 0.8860 - val_loss: 0.4721 - val_acc: 0.8630\n",
      "Epoch 4/20\n",
      "157/157 - 18s - loss: 0.3380 - acc: 0.9041 - val_loss: 0.8281 - val_acc: 0.8194\n",
      "Epoch 5/20\n",
      "157/157 - 18s - loss: 0.2935 - acc: 0.9229 - val_loss: 0.6939 - val_acc: 0.8426\n",
      "Epoch 6/20\n",
      "157/157 - 18s - loss: 0.2526 - acc: 0.9420 - val_loss: 0.6029 - val_acc: 0.8694\n",
      "Epoch 7/20\n",
      "157/157 - 18s - loss: 0.2060 - acc: 0.9568 - val_loss: 0.7363 - val_acc: 0.8606\n",
      "Epoch 8/20\n",
      "157/157 - 18s - loss: 0.1811 - acc: 0.9709 - val_loss: 0.8476 - val_acc: 0.8672\n",
      "Epoch 9/20\n",
      "157/157 - 17s - loss: 0.1553 - acc: 0.9795 - val_loss: 0.8411 - val_acc: 0.8656\n",
      "Epoch 10/20\n",
      "157/157 - 17s - loss: 0.1270 - acc: 0.9850 - val_loss: 0.9293 - val_acc: 0.8652\n",
      "Epoch 11/20\n",
      "157/157 - 18s - loss: 0.1119 - acc: 0.9900 - val_loss: 0.9557 - val_acc: 0.8704\n",
      "Epoch 12/20\n",
      "157/157 - 17s - loss: 0.0967 - acc: 0.9937 - val_loss: 0.9918 - val_acc: 0.8734\n",
      "Epoch 13/20\n",
      "157/157 - 17s - loss: 0.0952 - acc: 0.9937 - val_loss: 0.9725 - val_acc: 0.8720\n",
      "Epoch 14/20\n",
      "157/157 - 17s - loss: 0.0947 - acc: 0.9938 - val_loss: 0.9734 - val_acc: 0.8730\n",
      "Epoch 15/20\n",
      "157/157 - 17s - loss: 0.0945 - acc: 0.9938 - val_loss: 1.0013 - val_acc: 0.8706\n",
      "Epoch 16/20\n",
      "157/157 - 19s - loss: 0.0945 - acc: 0.9938 - val_loss: 0.9777 - val_acc: 0.8714\n",
      "Epoch 17/20\n",
      "157/157 - 19s - loss: 0.0944 - acc: 0.9938 - val_loss: 0.9856 - val_acc: 0.8730\n",
      "Epoch 18/20\n",
      "157/157 - 19s - loss: 0.0944 - acc: 0.9938 - val_loss: 1.0047 - val_acc: 0.8708\n",
      "Epoch 19/20\n",
      "157/157 - 19s - loss: 0.0944 - acc: 0.9938 - val_loss: 0.9794 - val_acc: 0.8706\n",
      "Epoch 20/20\n",
      "157/157 - 19s - loss: 0.0944 - acc: 0.9938 - val_loss: 0.9922 - val_acc: 0.8728\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=20, \n",
    "                    batch_size=128, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=callbacks, \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
